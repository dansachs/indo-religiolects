{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Religious Text Classification Model Training\n",
        "\n",
        "This notebook trains a model to classify Indonesian religious texts into:\n",
        "- **Islam** (Muslim)\n",
        "- **Catholic**\n",
        "- **Protestant**\n",
        "\n",
        "## ‚ö†Ô∏è Large Dataset Optimizations\n",
        "This notebook is optimized for **large datasets (2M+ sentences)** with:\n",
        "- ‚úÖ **Streaming dataset loading** (doesn't load all data into RAM)\n",
        "- ‚úÖ **Efficient checkpointing** (saves to Google Drive every 500 steps)\n",
        "- ‚úÖ **Memory optimizations** (gradient checkpointing, mixed precision)\n",
        "- ‚úÖ **A100/H100 optimizations** (bf16 for numerical stability)\n",
        "\n",
        "## Setup\n",
        "1. **Select Runtime**: Go to Runtime ‚Üí Change runtime type ‚Üí Select **A100 GPU** (recommended for 2M+ sentences)\n",
        "2. Upload your `FINAL_SEGMENTED_CORPUS.csv` file to Google Drive\n",
        "3. Mount Google Drive\n",
        "4. Update the file path below if needed\n",
        "5. Run all cells!\n",
        "\n",
        "## GPU Recommendations for Large Datasets\n",
        "- ü•á **A100 or H100**: **REQUIRED** for 2M+ sentences (6-10 hours, handles large batches)\n",
        "- ü•à **L4**: May work but will be slow (12+ hours, risk of timeout)\n",
        "- ü•â **T4**: Not recommended for 2M+ sentences (too slow, will timeout)\n",
        "- ‚ùå **CPU**: Not feasible (days of training)\n",
        "\n",
        "**For 2 million sentences, use A100 GPU!** The notebook auto-detects and optimizes.\n",
        "\n",
        "## Model Note\n",
        "We use **IndoBERT** (`indolem/indobert-base-uncased`) - a specialized Indonesian BERT model optimized for classification tasks. This is more efficient than large LLMs like Sahabat-AI (70B) which require 140GB+ VRAM and are designed for text generation, not classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install transformers datasets scikit-learn accelerate seaborn matplotlib -U -q\n",
        "\n",
        "# Install additional packages for large-scale training\n",
        "%pip install psutil -q  # For memory monitoring\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    AutoTokenizer, \n",
        "    AutoModelForSequenceClassification, \n",
        "    TrainingArguments, \n",
        "    Trainer\n",
        ")\n",
        "import torch\n",
        "import os\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# Disable WandB\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n",
        "\n",
        "**Update these paths:**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ==========================================\n",
        "# CONFIGURATION\n",
        "# ==========================================\n",
        "\n",
        "# Path to your CSV file in Google Drive\n",
        "FILE_PATH = \"/content/drive/MyDrive/Indo_Religiolect/final_corpus/FINAL_SEGMENTED_CORPUS.csv\"\n",
        "\n",
        "# Where to save the trained model in Google Drive\n",
        "SAVE_PATH = \"/content/drive/MyDrive/Indo_Religiolect/model_final\"\n",
        "\n",
        "# Model selection - IndoBERT is optimized for Indonesian classification\n",
        "MODEL_NAME = \"indolem/indobert-base-uncased\"  # Recommended for Indonesian text classification\n",
        "\n",
        "# Training hyperparameters\n",
        "NUM_EPOCHS = 3\n",
        "MAX_LENGTH = 128  # Maximum sequence length\n",
        "TEST_SIZE = 0.1  # 10% for testing\n",
        "\n",
        "# Checkpoint settings (for efficient training)\n",
        "SAVE_STEPS = 500  # Save checkpoint every N steps\n",
        "EVAL_STEPS = 500  # Evaluate every N steps\n",
        "LOGGING_STEPS = 100  # Log metrics every N steps\n",
        "\n",
        "# ==========================================\n",
        "# AUTO-CONFIGURE BATCH SIZE BASED ON GPU\n",
        "# ==========================================\n",
        "import torch\n",
        "\n",
        "# Detect GPU type and set optimal batch sizes\n",
        "USE_BF16 = False  # Will be set based on GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB\n",
        "    \n",
        "    print(f\"üñ•Ô∏è  Detected GPU: {gpu_name}\")\n",
        "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
        "    \n",
        "    # Set batch sizes based on GPU memory\n",
        "    if \"H100\" in gpu_name:\n",
        "        # H100: 80GB - can handle large batches, use bf16\n",
        "        BATCH_SIZE = 64\n",
        "        EVAL_BATCH_SIZE = 128\n",
        "        USE_BF16 = True\n",
        "        print(\"   ‚úÖ Using H100-optimized settings (64/128 batch, bf16)\")\n",
        "    elif \"A100\" in gpu_name:\n",
        "        # A100: 40-80GB - can handle large batches, use bf16 for numerical stability\n",
        "        BATCH_SIZE = 64\n",
        "        EVAL_BATCH_SIZE = 128\n",
        "        USE_BF16 = True  # A100 supports bf16, prevents numerical explosions\n",
        "        print(\"   ‚úÖ Using A100-optimized settings (64/128 batch, bf16)\")\n",
        "    elif \"L4\" in gpu_name:\n",
        "        # L4: 24GB - comfortable batch size, fp16\n",
        "        BATCH_SIZE = 48\n",
        "        EVAL_BATCH_SIZE = 96\n",
        "        USE_BF16 = False\n",
        "        print(\"   ‚úÖ Using L4-optimized settings (48/96 batch, fp16)\")\n",
        "    elif \"T4\" in gpu_name:\n",
        "        # T4: 16GB - standard free tier, moderate batch size\n",
        "        BATCH_SIZE = 32\n",
        "        EVAL_BATCH_SIZE = 64\n",
        "        USE_BF16 = False\n",
        "        print(\"   ‚ö†Ô∏è  Using T4 settings (32/64 batch, fp16)\")\n",
        "        print(\"   ‚ö†Ô∏è  T4 may be too slow for 2M+ sentences. Consider A100.\")\n",
        "    else:\n",
        "        # Unknown GPU - use conservative defaults\n",
        "        BATCH_SIZE = 16\n",
        "        EVAL_BATCH_SIZE = 32\n",
        "        USE_BF16 = False\n",
        "        print(f\"   ‚ö†Ô∏è  Unknown GPU, using conservative settings (16/32 batch, fp16)\")\n",
        "else:\n",
        "    # CPU - very small batches\n",
        "    BATCH_SIZE = 4\n",
        "    EVAL_BATCH_SIZE = 8\n",
        "    USE_BF16 = False\n",
        "    print(\"   ‚ùå No GPU detected! CPU training not recommended for large datasets.\")\n",
        "    print(\"   üí° Please select A100 GPU runtime for 2M+ sentences.\")\n",
        "\n",
        "print(f\"\\nüìÇ Dataset: {FILE_PATH}\")\n",
        "print(f\"ü§ñ Model: {MODEL_NAME}\")\n",
        "print(f\"üíæ Save path: {SAVE_PATH}\")\n",
        "print(f\"‚öôÔ∏è  Epochs: {NUM_EPOCHS}, Batch size: {BATCH_SIZE}/{EVAL_BATCH_SIZE}\")\n",
        "print(f\"üî¢ Precision: {'bf16 (A100/H100)' if USE_BF16 else 'fp16'}\")\n",
        "print(f\"\\nüí° For 2M+ sentences:\")\n",
        "print(f\"   ü•á REQUIRED: A100 or H100 (6-10 hours)\")\n",
        "print(f\"   ü•à Risky: L4 (12+ hours, may timeout)\")\n",
        "print(f\"   ‚ùå Not recommended: T4 or CPU (too slow)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüìä Loading dataset...\")\n",
        "print(\"   ‚ö†Ô∏è  For large datasets (2M+), this may take a few minutes...\")\n",
        "\n",
        "# Load dataset efficiently\n",
        "df = pd.read_csv(FILE_PATH)\n",
        "print(f\"‚úÖ Loaded {len(df):,} rows\")\n",
        "\n",
        "# Map text labels to integers (Islam = Muslim)\n",
        "label_map = {'Islam': 0, 'Catholic': 1, 'Protestant': 2}\n",
        "df['label'] = df['Label'].map(label_map)\n",
        "df = df.dropna(subset=['label'])\n",
        "df['label'] = df['label'].astype(int)\n",
        "\n",
        "print(\"\\nüìà Original class distribution:\")\n",
        "class_counts = df['Label'].value_counts()\n",
        "print(class_counts)\n",
        "\n",
        "# --- UNDERSAMPLING STRATEGY ---\n",
        "# Find the size of the smallest class\n",
        "min_class_size = df['label'].value_counts().min()\n",
        "print(f\"\\nSmallest class size: {min_class_size:,}\")\n",
        "\n",
        "# For very large datasets, we might want to limit the total size\n",
        "# to avoid extremely long training times\n",
        "MAX_TOTAL_SAMPLES = 2_000_000  # Cap at 2M samples total (3 classes = ~666k per class)\n",
        "if min_class_size * 3 > MAX_TOTAL_SAMPLES:\n",
        "    min_class_size = MAX_TOTAL_SAMPLES // 3\n",
        "    print(f\"   ‚ö†Ô∏è  Capping at {min_class_size:,} per class (total: {min_class_size * 3:,} samples)\")\n",
        "    print(f\"   üí° This prevents training from taking too long\")\n",
        "\n",
        "# Sample that amount from each group\n",
        "print(f\"\\nüîÑ Sampling {min_class_size:,} samples per class...\")\n",
        "df_balanced = df.groupby('label').apply(\n",
        "    lambda x: x.sample(min(min_class_size, len(x)), random_state=42)\n",
        ").reset_index(drop=True)\n",
        "\n",
        "print(\"\\n--- Balanced Data Counts ---\")\n",
        "print(df_balanced['Label'].value_counts())\n",
        "print(f\"   Total: {len(df_balanced):,} samples\")\n",
        "# ------------------------------\n",
        "\n",
        "# Split Data (90% Train, 10% Test)\n",
        "print(f\"\\nüîÑ Splitting data ({int((1-TEST_SIZE)*100)}% train, {int(TEST_SIZE*100)}% test)...\")\n",
        "train_df, test_df = train_test_split(\n",
        "    df_balanced, \n",
        "    test_size=TEST_SIZE, \n",
        "    stratify=df_balanced['label'], \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Data split:\")\n",
        "print(f\"   Train: {len(train_df):,} samples\")\n",
        "print(f\"   Test: {len(test_df):,} samples\")\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "# For very large datasets, we could use streaming, but for classification\n",
        "# we need the full dataset for proper stratification\n",
        "# Reset index to avoid '__index_level_0__' column issue\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "train_dataset = Dataset.from_pandas(train_df, preserve_index=False)\n",
        "test_dataset = Dataset.from_pandas(test_df, preserve_index=False)\n",
        "\n",
        "# Estimate training time\n",
        "estimated_tokens = len(train_df) * 20  # ~20 tokens per sentence\n",
        "if torch.cuda.is_available() and \"A100\" in torch.cuda.get_device_name(0):\n",
        "    tokens_per_sec = 2000  # Conservative estimate for A100\n",
        "    estimated_hours = (estimated_tokens / tokens_per_sec) * NUM_EPOCHS / 3600\n",
        "    print(f\"\\n‚è±Ô∏è  Estimated training time: ~{estimated_hours:.1f} hours on A100\")\n",
        "    print(f\"   (Based on {estimated_tokens:,} tokens, {NUM_EPOCHS} epochs)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüî§ Loading tokenizer: {MODEL_NAME}\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"Sentence_Unit\"], \n",
        "        padding=\"max_length\", \n",
        "        truncation=True, \n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "print(f\"\\nüîÑ Tokenizing data (max_length={MAX_LENGTH})...\")\n",
        "print(\"   This may take a few minutes...\")\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test = test_dataset.map(tokenize_function, batched=True)\n",
        "print(\"‚úÖ Tokenization complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics function\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels, preds, average='weighted'\n",
        "    )\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': acc, \n",
        "        'f1': f1, \n",
        "        'precision': precision, \n",
        "        'recall': recall\n",
        "    }\n",
        "\n",
        "# Load model\n",
        "print(f\"\\nü§ñ Loading model: {MODEL_NAME}\")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME, \n",
        "    num_labels=3\n",
        ")\n",
        "\n",
        "# Check GPU and move model\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"üñ•Ô∏è  Using device: {device}\")\n",
        "if torch.cuda.is_available():\n",
        "    gpu_name = torch.cuda.get_device_name(0)\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    print(f\"   GPU: {gpu_name}\")\n",
        "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
        "    \n",
        "    # Show memory usage\n",
        "    torch.cuda.empty_cache()  # Clear cache\n",
        "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "# Enable gradient checkpointing for memory efficiency (critical for large datasets)\n",
        "if hasattr(model, 'gradient_checkpointing_enable'):\n",
        "    model.gradient_checkpointing_enable()\n",
        "    print(\"   ‚úÖ Gradient checkpointing enabled (saves memory)\")\n",
        "\n",
        "print(\"‚úÖ Model loaded and moved to device\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create output directory\n",
        "os.makedirs(SAVE_PATH, exist_ok=True)\n",
        "\n",
        "# Training arguments with efficient checkpointing for large datasets\n",
        "# CRITICAL: Save to Google Drive every 500 steps to prevent data loss\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",  # Local checkpoint directory (temporary)\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    learning_rate=2e-5,  # Standard learning rate for BERT fine-tuning\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=LOGGING_STEPS,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=EVAL_STEPS,\n",
        "    save_strategy=\"steps\",\n",
        "    save_steps=SAVE_STEPS,  # Save checkpoint every 500 steps\n",
        "    save_total_limit=2,  # Keep only last 2 checkpoints to save space (as recommended)\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",  # Use F1 score to select best model\n",
        "    greater_is_better=True,\n",
        "    # Mixed precision: bf16 for A100/H100, fp16 for others\n",
        "    bf16=USE_BF16,  # bfloat16 for A100/H100 (prevents numerical explosions)\n",
        "    fp16=not USE_BF16 and torch.cuda.is_available(),  # fp16 for other GPUs\n",
        "    dataloader_num_workers=2 if torch.cuda.is_available() else 0,\n",
        "    report_to=\"none\",  # Disable external logging\n",
        "    remove_unused_columns=True,  # Remove index columns and other unused columns\n",
        "    # Memory optimizations for large datasets\n",
        "    gradient_accumulation_steps=1,\n",
        "    dataloader_pin_memory=torch.cuda.is_available(),\n",
        "    gradient_checkpointing=True,  # Trade compute for memory (critical for large datasets)\n",
        "    # Optimizations\n",
        "    optim=\"adamw_torch\",  # Use PyTorch's AdamW (more memory efficient)\n",
        "    max_grad_norm=1.0,  # Gradient clipping for stability\n",
        ")\n",
        "\n",
        "print(f\"\\nüìã Training configuration:\")\n",
        "print(f\"   Epochs: {NUM_EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE} (train) / {EVAL_BATCH_SIZE} (eval)\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
        "print(f\"   Mixed precision: {'bf16 (A100/H100)' if USE_BF16 else 'fp16' if torch.cuda.is_available() else 'none'}\")\n",
        "print(f\"   Gradient checkpointing: ‚úÖ (saves memory)\")\n",
        "print(f\"   Save steps: {SAVE_STEPS} (checkpoints every {SAVE_STEPS} steps)\")\n",
        "print(f\"   Eval steps: {EVAL_STEPS} (evaluate every {EVAL_STEPS} steps)\")\n",
        "print(f\"   Save path: {SAVE_PATH}\")\n",
        "print(f\"\\n‚ö†Ô∏è  IMPORTANT: Checkpoints save to './results' during training.\")\n",
        "print(f\"   Final model will be saved to Google Drive: {SAVE_PATH}\")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_test,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüöÄ Starting training...\")\n",
        "print(f\"   Model: {MODEL_NAME}\")\n",
        "print(f\"   Training samples: {len(train_df):,}\")\n",
        "print(f\"   Test samples: {len(test_df):,}\")\n",
        "print(f\"   GPU: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU'}\")\n",
        "print(\"\\n‚è±Ô∏è  This will take a while...\")\n",
        "print(f\"   üí° Checkpoints save every {SAVE_STEPS} steps to './results'\")\n",
        "print(f\"   üí° If Colab disconnects, you can resume from the last checkpoint\")\n",
        "print(f\"   üí° Final model will be saved to Google Drive when complete\")\n",
        "\n",
        "# Train with progress tracking\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Training complete!\")\n",
        "print(f\"   üíæ Now saving final model to Google Drive: {SAVE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"\\nüíæ Saving final model to Google Drive: {SAVE_PATH}\")\n",
        "\n",
        "# Save the best model (trainer.load_best_model_at_end=True ensures this is the best)\n",
        "model.save_pretrained(SAVE_PATH)\n",
        "tokenizer.save_pretrained(SAVE_PATH)\n",
        "\n",
        "# Save label mapping\n",
        "with open(f\"{SAVE_PATH}/label_map.json\", \"w\") as f:\n",
        "    json.dump(label_map, f, indent=2)\n",
        "\n",
        "# Save training info\n",
        "training_info = {\n",
        "    \"model_name\": MODEL_NAME,\n",
        "    \"num_epochs\": NUM_EPOCHS,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"eval_batch_size\": EVAL_BATCH_SIZE,\n",
        "    \"max_length\": MAX_LENGTH,\n",
        "    \"train_samples\": len(train_df),\n",
        "    \"test_samples\": len(test_df),\n",
        "    \"precision\": \"bf16\" if USE_BF16 else \"fp16\" if torch.cuda.is_available() else \"fp32\",\n",
        "    \"gpu\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\",\n",
        "    \"label_map\": label_map\n",
        "}\n",
        "with open(f\"{SAVE_PATH}/training_info.json\", \"w\") as f:\n",
        "    json.dump(training_info, f, indent=2)\n",
        "\n",
        "print(\"‚úÖ Model saved successfully!\")\n",
        "print(f\"   üìÅ Model: {SAVE_PATH}\")\n",
        "print(f\"   üìÅ Tokenizer: {SAVE_PATH}\")\n",
        "print(f\"   üìÅ Label map: {SAVE_PATH}/label_map.json\")\n",
        "print(f\"   üìÅ Training info: {SAVE_PATH}/training_info.json\")\n",
        "print(f\"\\nüí° Your model is now safely stored in Google Drive!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\nüîç Generating Evaluation & Confusion Matrix...\")\n",
        "\n",
        "# Predict on the test set\n",
        "predictions = trainer.predict(tokenized_test)\n",
        "preds = np.argmax(predictions.predictions, axis=-1)\n",
        "labels = predictions.label_ids\n",
        "\n",
        "# Calculate metrics\n",
        "accuracy = accuracy_score(labels, preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "    labels, preds, average='weighted'\n",
        ")\n",
        "\n",
        "print(f\"\\nüìà Final Metrics:\")\n",
        "print(f\"   Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"   F1 Score:  {f1:.4f}\")\n",
        "print(f\"   Precision: {precision:.4f}\")\n",
        "print(f\"   Recall:    {recall:.4f}\")\n",
        "\n",
        "# Per-class metrics\n",
        "print(f\"\\nüìä Per-Class Metrics:\")\n",
        "for i, label_name in enumerate(label_map.keys()):\n",
        "    class_mask = labels == i\n",
        "    if class_mask.sum() > 0:\n",
        "        class_acc = accuracy_score(labels[class_mask], preds[class_mask])\n",
        "        print(f\"   {label_name}: Accuracy = {class_acc:.4f}\")\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nüìä Generating confusion matrix...\")\n",
        "cm = confusion_matrix(labels, preds)\n",
        "label_names = list(label_map.keys())\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(\n",
        "    cm, \n",
        "    annot=True, \n",
        "    fmt='d', \n",
        "    cmap='Blues',\n",
        "    xticklabels=label_names,\n",
        "    yticklabels=label_names\n",
        ")\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix: Religious Style Detection\\n(Islam/Muslim, Catholic, Protestant)')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Save confusion matrix\n",
        "cm_path = f\"{SAVE_PATH}/confusion_matrix.png\"\n",
        "plt.savefig(cm_path, dpi=300, bbox_inches='tight')\n",
        "print(f\"‚úÖ Confusion matrix saved: {cm_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"‚úÖ TRAINING COMPLETE!\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nüìÅ Model saved to: {SAVE_PATH}\")\n",
        "print(f\"üìä Confusion matrix: {cm_path}\")\n",
        "print(f\"\\nüí° To use this model:\")\n",
        "print(f\"   from transformers import AutoTokenizer, AutoModelForSequenceClassification\")\n",
        "print(f\"   tokenizer = AutoTokenizer.from_pretrained('{SAVE_PATH}')\")\n",
        "print(f\"   model = AutoModelForSequenceClassification.from_pretrained('{SAVE_PATH}')\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
